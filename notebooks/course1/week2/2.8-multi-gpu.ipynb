{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.8: Multi-GPU Inference\n",
    "\n",
    "**Objective**: Load large models across multiple GPUs\n",
    "\n",
    "**Duration**: 20 minutes\n",
    "\n",
    "## Learning Outcomes\n",
    "- Use device_map=\"auto\" for automatic distribution\n",
    "- Understand memory estimation\n",
    "- Handle CPU offloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../../../src\")\n",
    "from hf_ecosystem import __version__\n",
    "print(f\"hf-ecosystem version: {__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from hf_ecosystem.inference import get_device, get_device_map, get_gpu_memory_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Check Available Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device()\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    memory = get_gpu_memory_info()\n",
    "    print(f\"GPU Memory: {memory['total']:.1f} GB total, {memory['free']:.1f} GB free\")\n",
    "else:\n",
    "    print(\"No GPU available - using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Automatic Device Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get device map for a ~500MB model\n",
    "device_map = get_device_map(model_size_gb=0.5)\n",
    "print(f\"Device map: {device_map}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": ["skip-ci"]
   },
   "outputs": [],
   "source": [
    "# Load model with automatic device mapping\n",
    "# Note: This cell may be slow on first run\n",
    "model_id = \"gpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=device_map if device == \"cuda\" else None,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    ")\n",
    "\n",
    "print(f\"Model loaded on: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Memory-Efficient Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": ["skip-ci"]
   },
   "outputs": [],
   "source": [
    "# Generate text\n",
    "inputs = tokenizer(\"The key to efficient ML is\", return_tensors=\"pt\")\n",
    "if device == \"cuda\":\n",
    "    inputs = inputs.to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_length=30)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_lab():\n",
    "    assert device in [\"cuda\", \"mps\", \"cpu\"]\n",
    "    assert device_map is not None\n",
    "    print(\"âœ… Lab completed successfully!\")\n",
    "\n",
    "verify_lab()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
