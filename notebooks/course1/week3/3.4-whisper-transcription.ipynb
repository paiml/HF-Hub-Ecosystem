{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.4: Speech-to-Text with Whisper\n",
    "\n",
    "**Objective**: Transcribe audio using OpenAI Whisper\n",
    "\n",
    "**Duration**: 25 minutes\n",
    "\n",
    "## Learning Outcomes\n",
    "- Load Whisper model for transcription\n",
    "- Handle audio files\n",
    "- Detect language automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../../../src\")\n",
    "from hf_ecosystem import __version__\n",
    "print(f\"hf-ecosystem version: {__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from transformers import WhisperProcessor, WhisperForConditionalGeneration\nfrom datasets import load_dataset"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Whisper Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load Whisper model and processor (manual inference due to pipeline bug in transformers 5.0)\nmodel = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")\nprocessor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\")\nprint(\"Whisper tiny model loaded\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Transcribe Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load sample audio from dataset\nds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation[:1]\")\naudio_decoder = ds[0][\"audio\"]\n\n# Extract audio data (new torchcodec API)\nsamples = audio_decoder.get_all_samples()\naudio = {\n    \"array\": samples.data.squeeze(0).numpy(),  # Convert to numpy, remove channel dim\n    \"sampling_rate\": samples.sample_rate,\n}\nprint(f\"Audio sampling rate: {audio['sampling_rate']} Hz\")\nprint(f\"Audio shape: {audio['array'].shape}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Transcribe using manual inference\ninputs = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], return_tensors=\"pt\")\ngenerated_ids = model.generate(inputs[\"input_features\"], language=\"en\", task=\"transcribe\")\nresult = {\"text\": processor.batch_decode(generated_ids, skip_special_tokens=True)[0]}\nprint(f\"Transcription: {result['text']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_lab():\n",
    "    assert \"text\" in result\n",
    "    assert len(result[\"text\"]) > 0\n",
    "    print(\"âœ… Lab completed successfully!\")\n",
    "\n",
    "verify_lab()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
