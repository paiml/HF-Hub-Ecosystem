{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.6: Image Captioning and VQA\n",
    "\n",
    "**Objective**: Generate captions and answer questions about images\n",
    "\n",
    "**Duration**: 30 minutes\n",
    "\n",
    "## Learning Outcomes\n",
    "- Use BLIP for image captioning\n",
    "- Perform visual question answering\n",
    "- Understand multi-modal model outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../../../src\")\n",
    "from hf_ecosystem import __version__\n",
    "print(f\"hf-ecosystem version: {__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from transformers import pipeline, BlipProcessor, BlipForConditionalGeneration\nfrom PIL import Image\nimport requests\nfrom io import BytesIO"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Image Captioning with BLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load BLIP model and processor (manual inference since pipeline API changed in transformers 5.0)\nblip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\nblip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\nprint(\"BLIP captioning model loaded\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load image (using HuggingFace docs which doesn't block requests)\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\nresponse = requests.get(url)\nimage = Image.open(BytesIO(response.content)).convert(\"RGB\")\n\n# Generate caption using manual inference\ninputs = blip_processor(image, return_tensors=\"pt\")\noutput = blip_model.generate(**inputs, max_new_tokens=50)\ncaption = [{\"generated_text\": blip_processor.decode(output[0], skip_special_tokens=True)}]\nprint(f\"Caption: {caption[0]['generated_text']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visual Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vqa = pipeline(\"visual-question-answering\", model=\"dandelin/vilt-b32-finetuned-vqa\", device=\"cpu\")\n",
    "print(\"VQA model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask questions about the image\n",
    "questions = [\"What color is prominent?\", \"Is this a photo?\"]\n",
    "\n",
    "for q in questions:\n",
    "    answer = vqa(image, q)\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"A: {answer[0]['answer']} ({answer[0]['score']:.3f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_lab():\n",
    "    assert len(caption) > 0\n",
    "    assert \"generated_text\" in caption[0]\n",
    "    print(\"âœ… Lab completed successfully!\")\n",
    "\n",
    "verify_lab()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
