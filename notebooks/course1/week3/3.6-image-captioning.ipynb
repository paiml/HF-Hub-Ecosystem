{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.6: Image Captioning and VQA\n",
    "\n",
    "**Objective**: Generate captions and answer questions about images\n",
    "\n",
    "**Duration**: 30 minutes\n",
    "\n",
    "## Learning Outcomes\n",
    "- Use BLIP for image captioning\n",
    "- Perform visual question answering\n",
    "- Understand multi-modal model outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../../../src\")\n",
    "from hf_ecosystem import __version__\n",
    "print(f\"hf-ecosystem version: {__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Image Captioning with BLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captioner = pipeline(\"image-to-text\", model=\"Salesforce/blip-image-captioning-base\", device=\"cpu\")\n",
    "print(\"BLIP captioning model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/47/PNG_transparency_demonstration_1.png/300px-PNG_transparency_demonstration_1.png\"\n",
    "response = requests.get(url)\n",
    "image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "\n",
    "# Generate caption\n",
    "caption = captioner(image)\n",
    "print(f\"Caption: {caption[0]['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visual Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vqa = pipeline(\"visual-question-answering\", model=\"dandelin/vilt-b32-finetuned-vqa\", device=\"cpu\")\n",
    "print(\"VQA model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask questions about the image\n",
    "questions = [\"What color is prominent?\", \"Is this a photo?\"]\n",
    "\n",
    "for q in questions:\n",
    "    answer = vqa(image, q)\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"A: {answer[0]['answer']} ({answer[0]['score']:.3f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_lab():\n",
    "    assert len(caption) > 0\n",
    "    assert \"generated_text\" in caption[0]\n",
    "    print(\"âœ… Lab completed successfully!\")\n",
    "\n",
    "verify_lab()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
